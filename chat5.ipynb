{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "from langchain.vectorstores  import Chroma\n",
    "from langchain.agents import Tool, tool\n",
    "import langchain\n",
    "from langchain.agents import AgentType\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import openai, os\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: dbChroma\n"
     ]
    }
   ],
   "source": [
    "persist_directory = \"dbChroma\"\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"serprolim_collection\",\n",
    "    embedding_function= OpenAIEmbeddings(),\n",
    "    persist_directory=persist_directory\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='La profundidad del pozo séptico dependerá de la profundidad a la cual se encuentre la capa de arena, en Santa Cruz, por lo general se encuentra a partir de los tres metros. Lo ideal es que su pozo séptico pueda atravesar por lo menos un metro en la capa de arena y si existiese una corriente de agua subterránea, que exista al menos un metro y medio entre ella y la base del pozo.', metadata={'source': '6-pl'}),\n",
       " Document(page_content='La duración de un pozo séptico depende de su tamaño y del número de usuarios a los cuales atiende. Una cámara séptica bien construida, almacenará normalmente sólidos que deberán limpierse periodicamente. Un pozo deberia tener una vida util entre 3 a 12 años.', metadata={'source': '7-pl'})]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Que profundidad debe tener un pozo?\"\n",
    "docs = vectordb.similarity_search(question,k=2)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat completion llm\n",
    "chat=ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    )\n",
    "\n",
    "# conversational memory\n",
    "# memory = ConversationBufferMemory(\n",
    "#     memory_key=\"chat_history\", \n",
    "#     return_messages=True\n",
    "#     )\n",
    "\n",
    "# # retrieval augmented pipeline for chatbot\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=chat,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever = vectordb.as_retriever(search_kwargs={\"k\":2})\n",
    "# )\n",
    "\n",
    "chat.predict(\"Que profundidad debe terner un pozo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La profundidad de un pozo puede variar dependiendo de su propósito y ubicación. Algunos factores a considerar son:\\n\\n1. Uso del pozo: Si el pozo se utiliza para abastecer agua potable, la profundidad debe ser suficiente para alcanzar un acuífero subterráneo que proporcione agua de calidad. Esto puede variar según la región y la disponibilidad de agua subterránea.\\n\\n2. Ubicación geográfica: La profundidad del pozo también puede depender de la ubicación geográfica y las características del suelo. En algunas áreas, el agua subterránea puede estar más cerca de la superficie, mientras que en otras puede estar a mayor profundidad.\\n\\n3. Tipo de suelo: El tipo de suelo también puede influir en la profundidad del pozo. Algunos suelos pueden retener más agua, lo que puede requerir una menor profundidad del pozo, mientras que otros suelos pueden ser más permeables y requerir una mayor profundidad para alcanzar el agua subterránea.\\n\\nEn general, los pozos de agua potable suelen tener una profundidad promedio de entre 30 y 100 metros, pero esto puede variar significativamente según los factores mencionados anteriormente. Es importante consultar con expertos en perforación de pozos y regulaciones locales para determinar la profundidad adecuada para un pozo específico.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(str,k):\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\":k})\n",
    "    context = retriever.get_relevant_documents(str)\n",
    "    page_contents = [item.page_content for item in context]\n",
    "    combined_content = \". \".join(page_contents)\n",
    "    # Imprimir el resultado\n",
    "    # print(combined_content)\n",
    "    return combined_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    ")\n",
    "query = \"Que servicios dan?\"\n",
    "\n",
    "context = get_context(query,2)\n",
    "\n",
    "template  = \"\"\"\n",
    "Eres un asistente de atención al cliente para una empresa especializada en la limpieza de pozos y cámaras sépticas (pozos ciegos). \\\n",
    "Utiliza los siguientes fragmentos de contexto para responder la pregunta al final. \\\n",
    "Si no sabes la respuesta, simplemente di que no lo sabes, NO intentes inventar una respuesta. \\\n",
    "Utiliza un máximo de dos oraciones. Mantén la respuesta lo más concisa posible.\n",
    "\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    "    )\n",
    "\n",
    "# ai_template = AIMessagePromptTemplate.from_template(\"{response}\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(human_template)\n",
    "    ]\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "resp = chat([HumanMessage(content=query)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='\\nEres un asistente de atención al cliente para una empresa especializada en la limpieza de pozos sépticos y cámaras sépticas (pozos ciegos).     Toda la información necesaria está contenida en el contexto delimitado por los signos de triple igualdad.     Por favor, abstente de utilizar recursos externos, enlaces o referencias que no estén presentes en el contexto.     No te basarás en tu propio conocimiento sobre el tema. Si la respuesta no se encuentra en el contexto,     simplemente responde con \"Hmm, no estoy seguro/a\". No intentes inventar una respuesta.\\n===\\nDamos el servicio de limpieza de pozos sépticos, pozos ciegos y cámaras sépticas.. Podemos mejorar el precio en función a la frecuencia que con la que nos solicite el servicio y la cantidad de servicios que prestemos en ese momento en su zona o barrio. Si le es posible, coordine con sus vecinos y podrá obtener un precio más bajo. Tenemos descuentos para Condominios.\\n===\\n', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='{input}', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Como modelo de lenguaje de IA, no proporciono servicios directamente. Sin embargo, puedo ayudarte a responder preguntas, proporcionar información y asistirte en diversas tareas. Algunos ejemplos de servicios que puedo ofrecer son:\\n\\n- Responder preguntas generales sobre una ampl' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print (resp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format with required inputs\n",
    "chat_prompt_value = chat_prompt.format_prompt(\n",
    "    context=combined_content,\n",
    "    input=\"Como te llamas?\",\n",
    ")\n",
    "chat_prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "messages = chat_prompt_value.to_messages()\n",
    "\n",
    "messages.append(\n",
    "    HumanMessage(content=\"Que profundidad debe tener el pozo?\")\n",
    ")\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecioTool(BaseTool):\n",
    "    name = \"Calcula el precio\"\n",
    "    description = \"Useful when you need to answer questions about prices or the cost of the service.\"\n",
    "\n",
    "    def _run(self, ubicacion: str) -> str:\n",
    "        return \"Bs.300\"\n",
    "    \n",
    "    def _arun(self, ubicacion: str) -> str:\n",
    "        # return \"Bs.400\"\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "    \n",
    "# @tool\n",
    "# def reverse_string(query: str) -> str:\n",
    "#     '''Reverses a string.'''\n",
    "#     return query[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval qa chain\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                 retriever=retriever,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 verbose=True,\n",
    "                                 memory=memory,\n",
    "                                 combine_docs_chain_kwargs={'prompt': qa_prompt}\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    PrecioTool(),\n",
    "    Tool(\n",
    "        name=\"Knowledge Base\",\n",
    "        func=qa.run,\n",
    "        description=\"Useful when you need to answer general knowledge. If you don't know the answer, just say that you don't know, don't try to make up an answer.\",\n",
    "    ),\n",
    "    # reverse_string\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize agent with tools\n",
    "agent = initialize_agent(\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=2,\n",
    "    early_stopping_method='generate',\n",
    "    memory=memory,\n",
    "    # handle_parsing_errors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.verbose=True\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response = agent.run(\"Que profundidad debe tener un pozo?\")\n",
    "    print(response)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "\n",
    "# response = agent.run(\"Cyuanto cuesta el serivio?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent.agent.llm_chain.prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
